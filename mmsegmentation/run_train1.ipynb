{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bcec556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 26306 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "0/288 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "288/288 04:59<00:00 , loss=1.76, lr=0.000497, step=288\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 880, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 880, in <module>\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 880, in <module>\n",
      "    run(model, get_loaders)\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 805, in run\n",
      "    score = valid_one_epoch(f\"{OUTPUT_FOLDER}/{CFG.FOLD}.pth\", valid_loader, debug=False, running_dist=True)\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 661, in valid_one_epoch\n",
      "    model = Model(pre=None)\n",
      "TypeError: __init__() got an unexpected keyword argument 'pre'\n",
      "    run(model, get_loaders)\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 805, in run\n",
      "    run(model, get_loaders)    \n",
      "score = valid_one_epoch(f\"{OUTPUT_FOLDER}/{CFG.FOLD}.pth\", valid_loader, debug=False, running_dist=True)  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 805, in run\n",
      "\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 661, in valid_one_epoch\n",
      "    model = Model(pre=None)\n",
      "TypeError: __init__() got an unexpected keyword argument 'pre'\n",
      "    score = valid_one_epoch(f\"{OUTPUT_FOLDER}/{CFG.FOLD}.pth\", valid_loader, debug=False, running_dist=True)\n",
      "  File \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py\", line 661, in valid_one_epoch\n",
      "    model = Model(pre=None)\n",
      "TypeError: __init__() got an unexpected keyword argument 'pre'\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1457309) of binary: /home/harshit/anaconda3/envs/dacl10k_mmseg_t1/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py\", line 195, in <module>\n",
      "    main()\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py\", line 191, in main\n",
      "    launch(args)\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py\", line 176, in launch\n",
      "    run(args)\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/run.py\", line 753, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG//convnext_large_exp_002_forward_v1/run.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2023-10-28_15:41:10\n",
      "  host      : harshit\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 1457310)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2023-10-28_15:41:10\n",
      "  host      : harshit\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 1457311)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-10-28_15:41:10\n",
      "  host      : harshit\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1457309)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"convnext_large_exp_002_forward\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247db91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e449884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 26299 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "0/288 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "288/288 04:59<00:00 , loss=1.76, lr=0.000497, step=288\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 1 | mIOU 0.060806140204991165\n",
      "SAVING BEST!\n",
      "288/288 04:58<00:00 , loss=1.32, lr=0.00049, step=576 \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 2 | mIOU 0.21704828461392522\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=1.19, lr=0.000477, step=864\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 3 | mIOU 0.3022460493931551\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=1.1, lr=0.00046, step=1152  \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 4 | mIOU 0.3176942965912219\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=1.05, lr=0.000439, step=1440\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 5 | mIOU 0.33032271543499847\n",
      "SAVING BEST!\n",
      "288/288 04:58<00:00 , loss=1.04, lr=0.000414, step=1728\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:51<00:00 \n",
      "EPOCH 6 | mIOU 0.3336268909231077\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=0.992, lr=0.000385, step=2016\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 7 | mIOU 0.33757510963974163\n",
      "SAVING BEST!\n",
      "288/288 04:58<00:00 , loss=1.02, lr=0.000354, step=2304\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 8 | mIOU 0.34104606054915154\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=0.99, lr=0.00032, step=2592  \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 9 | mIOU 0.34224323246785526\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=0.951, lr=0.000286, step=2880\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 10 | mIOU 0.3457673150642039\n",
      "SAVING BEST!\n",
      "288/288 04:57<00:00 , loss=0.975, lr=0.00025, step=3168 \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 11 | mIOU 0.34984930249010304\n",
      "SAVING BEST!\n",
      "288/288 04:59<00:00 , loss=1, lr=0.000214, step=3456    \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:55<00:00 \n",
      "EPOCH 12 | mIOU 0.34814509882726336\n",
      "288/288 04:59<00:00 , loss=0.995, lr=0.00018, step=3744 \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 13 | mIOU 0.3522116783890334\n",
      "SAVING BEST!\n",
      "288/288 04:58<00:00 , loss=0.957, lr=0.000146, step=4032\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 14 | mIOU 0.35324689257349756\n",
      "SAVING BEST!\n",
      "288/288 04:58<00:00 , loss=0.944, lr=0.000115, step=4320\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 15 | mIOU 0.3525213418072293\n",
      "288/288 04:59<00:00 , loss=0.943, lr=8.63e-5, step=4608 \n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 16 | mIOU 0.3544494543675275\n",
      "SAVING BEST!\n",
      "288/288 04:59<00:00 , loss=0.939, lr=6.11e-5, step=4896\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 17 | mIOU 0.35362853475645334\n",
      "288/288 04:59<00:00 , loss=0.949, lr=3.97e-5, step=5184\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 18 | mIOU 0.3548029143803738\n",
      "SAVING BEST!\n",
      "74/288 01:18<03:41 , loss=0.937, lr=3.49e-5, step=5258"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"convnext_large_exp_002_forward\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a6b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08ccf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 26427 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "0/288 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "288/288 04:57<00:00 , loss=1.2, lr=0.000499, step=288 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 1 | mIOU 0.31023952299446095\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=1.01, lr=0.000497, step=576\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 2 | mIOU 0.3268852780127159\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.968, lr=0.000494, step=864\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 3 | mIOU 0.3308548591223396\n",
      "SAVING BEST!\n",
      "288/288 04:54<00:00 , loss=0.932, lr=0.00049, step=1152 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 4 | mIOU 0.33784930733576013\n",
      "SAVING BEST!\n",
      "288/288 04:54<00:00 , loss=0.912, lr=0.000484, step=1440\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 5 | mIOU 0.33862617594468397\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.903, lr=0.000477, step=1728\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 6 | mIOU 0.34370071952804043\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.872, lr=0.000469, step=2016\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 7 | mIOU 0.3411385633106409\n",
      "288/288 04:55<00:00 , loss=0.903, lr=0.00046, step=2304 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 8 | mIOU 0.3418930043640556\n",
      "288/288 04:55<00:00 , loss=0.854, lr=0.00045, step=2592 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 9 | mIOU 0.3489690081101413\n",
      "SAVING BEST!\n",
      "288/288 04:54<00:00 , loss=0.839, lr=0.000439, step=2880\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 10 | mIOU 0.34818106127888787\n",
      "288/288 04:56<00:00 , loss=0.849, lr=0.000427, step=3168\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 11 | mIOU 0.35380321305387513\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.866, lr=0.000414, step=3456\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 12 | mIOU 0.351269566028419\n",
      "288/288 04:55<00:00 , loss=0.866, lr=0.0004, step=3744  \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 13 | mIOU 0.3553298075440546\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.836, lr=0.000385, step=4032\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 14 | mIOU 0.35317066528579927\n",
      "288/288 04:55<00:00 , loss=0.813, lr=0.00037, step=4320 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 15 | mIOU 0.3596330503621561\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.82, lr=0.000354, step=4608 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 16 | mIOU 0.35528747917678694\n",
      "288/288 04:56<00:00 , loss=0.822, lr=0.000337, step=4896\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 17 | mIOU 0.3642858541727041\n",
      "SAVING BEST!\n",
      "288/288 04:56<00:00 , loss=0.825, lr=0.00032, step=5184 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 18 | mIOU 0.362981945704116\n",
      "288/288 04:56<00:00 , loss=0.788, lr=0.000303, step=5472\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 19 | mIOU 0.3615463947907141\n",
      "288/288 04:55<00:00 , loss=0.807, lr=0.000286, step=5760\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 20 | mIOU 0.362334703926069\n",
      "288/288 04:56<00:00 , loss=0.807, lr=0.000268, step=6048\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 21 | mIOU 0.36296971513286014\n",
      "288/288 04:56<00:00 , loss=0.784, lr=0.00025, step=6336 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 22 | mIOU 0.36253234207622853\n",
      "288/288 04:55<00:00 , loss=0.777, lr=0.000232, step=6624\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 23 | mIOU 0.36498530964447395\n",
      "SAVING BEST!\n",
      "288/288 04:54<00:00 , loss=0.751, lr=0.000214, step=6912\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 24 | mIOU 0.36890198955867587\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.789, lr=0.000197, step=7200\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 25 | mIOU 0.3684976922666817\n",
      "288/288 04:55<00:00 , loss=0.798, lr=0.00018, step=7488 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 26 | mIOU 0.3710251670646374\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.771, lr=0.000163, step=7776\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 27 | mIOU 0.37429154560087624\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.78, lr=0.000146, step=8064 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 28 | mIOU 0.3710628224175407\n",
      "288/288 04:57<00:00 , loss=0.789, lr=0.00013, step=8352 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 29 | mIOU 0.37212509789852444\n",
      "288/288 04:58<00:00 , loss=0.774, lr=0.000115, step=8640\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 30 | mIOU 0.3723947393688964\n",
      "288/288 04:57<00:00 , loss=0.764, lr=0.0001, step=8928  \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 31 | mIOU 0.3712495869567708\n",
      "288/288 04:55<00:00 , loss=0.738, lr=8.63e-5, step=9216\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 32 | mIOU 0.37442647931408474\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.785, lr=7.32e-5, step=9504\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 33 | mIOU 0.37296659307914687\n",
      "288/288 04:56<00:00 , loss=0.746, lr=6.11e-5, step=9792\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 34 | mIOU 0.37411731823073996\n",
      "288/288 04:55<00:00 , loss=0.748, lr=4.99e-5, step=10080\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 35 | mIOU 0.3737956662346846\n",
      "288/288 04:56<00:00 , loss=0.777, lr=3.97e-5, step=10368\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 36 | mIOU 0.3747860699856063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.742, lr=3.06e-5, step=10656\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 37 | mIOU 0.37436467951077856\n",
      "288/288 04:55<00:00 , loss=0.726, lr=2.26e-5, step=10944\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 38 | mIOU 0.3751007006987292\n",
      "SAVING BEST!\n",
      "288/288 04:55<00:00 , loss=0.749, lr=1.58e-5, step=11232\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 39 | mIOU 0.37470216872686823\n",
      "288/288 04:55<00:00 , loss=0.74, lr=1.01e-5, step=11520 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 40 | mIOU 0.37562227733488\n",
      "SAVING BEST!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"convnext_large_exp_002_forward\"\n",
    "V = \"2\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ed407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "662183a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 27293 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "0/385 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "385/385 07:21<00:00 , loss=1.1, lr=0.000499, step=385 \n",
      "47/47 01:42<00:00 \n",
      "EPOCH 1 | mIOU 0.31919829463209465\n",
      "SAVING BEST!\n",
      "385/385 07:22<00:00 , loss=0.945, lr=0.000497, step=770\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 2 | mIOU 0.3334636162678882\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.885, lr=0.000494, step=1155\n",
      "47/47 01:42<00:00 \n",
      "EPOCH 3 | mIOU 0.33672358444913053\n",
      "SAVING BEST!\n",
      "385/385 07:25<00:00 , loss=0.874, lr=0.00049, step=1540 \n",
      "47/47 01:42<00:00 \n",
      "EPOCH 4 | mIOU 0.3460727672471879\n",
      "SAVING BEST!\n",
      "385/385 07:23<00:00 , loss=0.833, lr=0.000484, step=1925\n",
      "47/47 01:43<00:00 \n",
      "EPOCH 5 | mIOU 0.3468211383458039\n",
      "SAVING BEST!\n",
      "385/385 07:24<00:00 , loss=0.837, lr=0.000477, step=2310\n",
      "47/47 01:42<00:00 \n",
      "EPOCH 6 | mIOU 0.3417589040604445\n",
      "385/385 07:22<00:00 , loss=0.831, lr=0.000469, step=2695\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 7 | mIOU 0.34620589869839496\n",
      "385/385 07:24<00:00 , loss=0.819, lr=0.00046, step=3080 \n",
      "47/47 01:42<00:00 \n",
      "EPOCH 8 | mIOU 0.34915631034052075\n",
      "SAVING BEST!\n",
      "385/385 07:25<00:00 , loss=0.801, lr=0.00045, step=3465 \n",
      "47/47 01:43<00:00 \n",
      "EPOCH 9 | mIOU 0.35128201760553296\n",
      "SAVING BEST!\n",
      "385/385 07:25<00:00 , loss=0.798, lr=0.000439, step=3850\n",
      "47/47 01:42<00:00 \n",
      "EPOCH 10 | mIOU 0.3592200849599217\n",
      "SAVING BEST!\n",
      "385/385 07:25<00:00 , loss=0.82, lr=0.000427, step=4235 \n",
      "47/47 01:42<00:00 \n",
      "EPOCH 11 | mIOU 0.35747303264523256\n",
      "385/385 07:25<00:00 , loss=0.781, lr=0.000414, step=4620\n",
      "47/47 01:43<00:00 \n",
      "EPOCH 12 | mIOU 0.3553278051528059\n",
      "385/385 07:25<00:00 , loss=0.781, lr=0.0004, step=5005  \n",
      "47/47 01:42<00:00 \n",
      "EPOCH 13 | mIOU 0.3569862142253865\n",
      "385/385 07:23<00:00 , loss=0.754, lr=0.000385, step=5390\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 14 | mIOU 0.35462795601147906\n",
      "385/385 07:22<00:00 , loss=0.784, lr=0.00037, step=5775 \n",
      "47/47 01:41<00:00 \n",
      "EPOCH 15 | mIOU 0.3597725261797544\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.776, lr=0.000354, step=6160\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 16 | mIOU 0.361980946223324\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.744, lr=0.000337, step=6545\n",
      "47/47 01:42<00:00 \n",
      "EPOCH 17 | mIOU 0.36714805214081014\n",
      "SAVING BEST!\n",
      "385/385 07:20<00:00 , loss=0.725, lr=0.00032, step=6930 \n",
      "47/47 01:42<00:00 \n",
      "EPOCH 18 | mIOU 0.36503529196192097\n",
      "385/385 07:22<00:00 , loss=0.772, lr=0.000303, step=7315\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 19 | mIOU 0.3611307412063029\n",
      "385/385 07:22<00:00 , loss=0.739, lr=0.000286, step=7700\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 20 | mIOU 0.3618251840267977\n",
      "385/385 07:22<00:00 , loss=0.765, lr=0.000268, step=8085\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 21 | mIOU 0.3683269863640175\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.731, lr=0.00025, step=8470 \n",
      "47/47 01:41<00:00 \n",
      "EPOCH 22 | mIOU 0.3692772636928211\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.756, lr=0.000232, step=8855\n",
      "47/47 01:42<00:00 \n",
      "EPOCH 23 | mIOU 0.36662734484725384\n",
      "385/385 07:21<00:00 , loss=0.7, lr=0.000214, step=9240  \n",
      "47/47 01:41<00:00 \n",
      "EPOCH 24 | mIOU 0.3698982273333802\n",
      "SAVING BEST!\n",
      "385/385 07:22<00:00 , loss=0.711, lr=0.000197, step=9625\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 25 | mIOU 0.37200166886310754\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.723, lr=0.00018, step=1e+4 \n",
      "47/47 01:41<00:00 \n",
      "EPOCH 26 | mIOU 0.3715038641410295\n",
      "385/385 07:22<00:00 , loss=0.716, lr=0.000163, step=10395\n",
      "47/47 01:40<00:00 \n",
      "EPOCH 27 | mIOU 0.37394264662296234\n",
      "SAVING BEST!\n",
      "385/385 07:20<00:00 , loss=0.692, lr=0.000146, step=10780\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 28 | mIOU 0.3738885140882113\n",
      "385/385 07:21<00:00 , loss=0.702, lr=0.00013, step=11165 \n",
      "47/47 01:41<00:00 \n",
      "EPOCH 29 | mIOU 0.3755416081966067\n",
      "SAVING BEST!\n",
      "385/385 07:22<00:00 , loss=0.708, lr=0.000115, step=11550\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 30 | mIOU 0.37524659312455333\n",
      "385/385 07:21<00:00 , loss=0.681, lr=0.0001, step=11935  \n",
      "47/47 01:40<00:00 \n",
      "EPOCH 31 | mIOU 0.3761859563483585\n",
      "SAVING BEST!\n",
      "385/385 07:22<00:00 , loss=0.709, lr=8.63e-5, step=12320\n",
      "47/47 01:22<00:00 \n",
      "EPOCH 32 | mIOU 0.37625644773511346\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.691, lr=7.32e-5, step=12705\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 33 | mIOU 0.37525986441701004\n",
      "385/385 07:21<00:00 , loss=0.684, lr=6.11e-5, step=13090\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 34 | mIOU 0.3755970166603464\n",
      "385/385 07:22<00:00 , loss=0.697, lr=4.99e-5, step=13475\n",
      "47/47 01:43<00:00 \n",
      "EPOCH 35 | mIOU 0.37484809383287165\n",
      "385/385 07:22<00:00 , loss=0.713, lr=3.97e-5, step=13860\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 36 | mIOU 0.3756609137401023\n",
      "385/385 07:21<00:00 , loss=0.694, lr=3.06e-5, step=14245\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 37 | mIOU 0.376262971221005\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.693, lr=2.26e-5, step=14630\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 38 | mIOU 0.3772253755256868\n",
      "SAVING BEST!\n",
      "385/385 07:21<00:00 , loss=0.693, lr=1.58e-5, step=15015\n",
      "47/47 01:42<00:00 \n",
      "EPOCH 39 | mIOU 0.3764916818908626\n",
      "385/385 07:21<00:00 , loss=0.673, lr=1.01e-5, step=15400\n",
      "47/47 01:41<00:00 \n",
      "EPOCH 40 | mIOU 0.37645684391115963\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"convnext_large_exp_002_forward\"\n",
    "V = \"3\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48b4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1996184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 28322 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "0/385 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "385/385 06:48<00:00 , loss=1.12, lr=0.000499, step=385\n",
      "47/47 01:07<00:00 \n",
      "EPOCH 1 | mIOU 0.31046591877272717\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.94, lr=0.000497, step=770 \n",
      "47/47 01:07<00:00 \n",
      "EPOCH 2 | mIOU 0.32202151077582436\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.875, lr=0.000494, step=1155\n",
      "47/47 01:05<00:00 \n",
      "EPOCH 3 | mIOU 0.3390434531189988\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.868, lr=0.00049, step=1540 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 4 | mIOU 0.3374849140198488\n",
      "385/385 06:49<00:00 , loss=0.837, lr=0.000484, step=1925\n",
      "47/47 00:59<00:00 \n",
      "EPOCH 5 | mIOU 0.34370188354180153\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.822, lr=0.000477, step=2310\n",
      "47/47 01:01<00:00 \n",
      "EPOCH 6 | mIOU 0.3385490837314923\n",
      "385/385 06:49<00:00 , loss=0.817, lr=0.000469, step=2695\n",
      "47/47 01:01<00:00 \n",
      "EPOCH 7 | mIOU 0.34897015653966496\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.815, lr=0.00046, step=3080 \n",
      "47/47 00:59<00:00 \n",
      "EPOCH 8 | mIOU 0.35113027881364756\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.79, lr=0.00045, step=3465  \n",
      "47/47 01:33<00:00 \n",
      "EPOCH 9 | mIOU 0.3507628357018347\n",
      "385/385 06:51<00:00 , loss=0.795, lr=0.000439, step=3850\n",
      "47/47 00:56<00:00 \n",
      "EPOCH 10 | mIOU 0.3578789452225167\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.796, lr=0.000427, step=4235\n",
      "47/47 00:58<00:00 \n",
      "EPOCH 11 | mIOU 0.3579645863356651\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.785, lr=0.000414, step=4620\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 12 | mIOU 0.3482163158855783\n",
      "385/385 06:48<00:00 , loss=0.773, lr=0.0004, step=5005  \n",
      "47/47 01:10<00:00 \n",
      "EPOCH 13 | mIOU 0.35718001997487336\n",
      "385/385 06:51<00:00 , loss=0.757, lr=0.000385, step=5390\n",
      "47/47 01:12<00:00 \n",
      "EPOCH 14 | mIOU 0.36050020289346096\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.766, lr=0.00037, step=5775 \n",
      "47/47 01:18<00:00 \n",
      "EPOCH 15 | mIOU 0.36462700655078983\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.76, lr=0.000354, step=6160 \n",
      "47/47 00:59<00:00 \n",
      "EPOCH 16 | mIOU 0.36680765502935186\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.734, lr=0.000337, step=6545\n",
      "47/47 00:55<00:00 \n",
      "EPOCH 17 | mIOU 0.3684238512102719\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.717, lr=0.00032, step=6930 \n",
      "47/47 01:17<00:00 \n",
      "EPOCH 18 | mIOU 0.3712435817514441\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.755, lr=0.000303, step=7315\n",
      "47/47 00:55<00:00 \n",
      "EPOCH 19 | mIOU 0.36621611970488704\n",
      "385/385 06:51<00:00 , loss=0.733, lr=0.000286, step=7700\n",
      "47/47 00:56<00:00 \n",
      "EPOCH 20 | mIOU 0.3652137340969477\n",
      "385/385 06:52<00:00 , loss=0.753, lr=0.000268, step=8085\n",
      "47/47 02:39<00:00 \n",
      "EPOCH 21 | mIOU 0.3686824828733206\n",
      "385/385 06:54<00:00 , loss=0.724, lr=0.00025, step=8470 \n",
      "47/47 01:24<00:00 \n",
      "EPOCH 22 | mIOU 0.370034461461562\n",
      "385/385 06:55<00:00 , loss=0.733, lr=0.000232, step=8855\n",
      "47/47 01:15<00:00 \n",
      "EPOCH 23 | mIOU 0.370319849021617\n",
      "385/385 06:55<00:00 , loss=0.7, lr=0.000214, step=9240  \n",
      "47/47 01:16<00:00 \n",
      "EPOCH 24 | mIOU 0.3735717532479097\n",
      "SAVING BEST!\n",
      "385/385 06:54<00:00 , loss=0.696, lr=0.000197, step=9625\n",
      "47/47 01:17<00:00 \n",
      "EPOCH 25 | mIOU 0.37170684847835767\n",
      "385/385 06:56<00:00 , loss=0.706, lr=0.00018, step=1e+4 \n",
      "47/47 01:11<00:00 \n",
      "EPOCH 26 | mIOU 0.37492809828106444\n",
      "SAVING BEST!\n",
      "385/385 06:55<00:00 , loss=0.69, lr=0.000163, step=10395 \n",
      "47/47 01:15<00:00 \n",
      "EPOCH 27 | mIOU 0.3769323272357916\n",
      "SAVING BEST!\n",
      "385/385 06:54<00:00 , loss=0.68, lr=0.000146, step=10780 \n",
      "47/47 01:11<00:00 \n",
      "EPOCH 28 | mIOU 0.3775247674790489\n",
      "SAVING BEST!\n",
      "385/385 06:51<00:00 , loss=0.695, lr=0.00013, step=11165 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 29 | mIOU 0.3779279800567139\n",
      "SAVING BEST!\n",
      "385/385 06:51<00:00 , loss=0.688, lr=0.000115, step=11550\n",
      "47/47 01:10<00:00 \n",
      "EPOCH 30 | mIOU 0.3775837124609986\n",
      "385/385 06:52<00:00 , loss=0.673, lr=0.0001, step=11935  \n",
      "47/47 01:14<00:00 \n",
      "EPOCH 31 | mIOU 0.37813076103270943\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.689, lr=8.63e-5, step=12320\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 32 | mIOU 0.3772320648502639\n",
      "385/385 06:51<00:00 , loss=0.681, lr=7.32e-5, step=12705\n",
      "47/47 01:14<00:00 \n",
      "EPOCH 33 | mIOU 0.378952295317347\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.665, lr=6.11e-5, step=13090\n",
      "47/47 01:05<00:00 \n",
      "EPOCH 34 | mIOU 0.3803995886081276\n",
      "SAVING BEST!\n",
      "385/385 06:50<00:00 , loss=0.684, lr=4.99e-5, step=13475\n",
      "47/47 01:12<00:00 \n",
      "EPOCH 35 | mIOU 0.3815308345555018\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.682, lr=3.97e-5, step=13860\n",
      "47/47 01:12<00:00 \n",
      "EPOCH 36 | mIOU 0.3812552039667418\n",
      "385/385 06:50<00:00 , loss=0.663, lr=3.06e-5, step=14245\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 37 | mIOU 0.38151855240851035\n",
      "385/385 06:51<00:00 , loss=0.674, lr=2.26e-5, step=14630\n",
      "47/47 01:14<00:00 \n",
      "EPOCH 38 | mIOU 0.38171075463784104\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.679, lr=1.58e-5, step=15015\n",
      "47/47 01:10<00:00 \n",
      "EPOCH 39 | mIOU 0.3812451502845758\n",
      "385/385 06:51<00:00 , loss=0.655, lr=1.01e-5, step=15400\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 40 | mIOU 0.3819057122555684\n",
      "SAVING BEST!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"convnext_large_exp_002_forward\"\n",
    "V = \"5\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6eae74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd7fc00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 28323 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/mmsegmentation/work_dirs/convnext_large_exp_002/best_mIoU_iter_45045.pth\n",
      "0/385 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "385/385 06:50<00:00 , loss=1.11, lr=0.000499, step=385\n",
      "47/47 01:03<00:00 \n",
      "EPOCH 1 | mIOU 0.3124657896194035\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.941, lr=0.000497, step=770\n",
      "47/47 01:07<00:00 \n",
      "EPOCH 2 | mIOU 0.32705379343541247\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.874, lr=0.000494, step=1155\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 3 | mIOU 0.3331720857318292\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.873, lr=0.00049, step=1540 \n",
      "47/47 01:16<00:00 \n",
      "EPOCH 4 | mIOU 0.3369841408109482\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.831, lr=0.000484, step=1925\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 5 | mIOU 0.3392266689709595\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.829, lr=0.000477, step=2310\n",
      "47/47 01:08<00:00 \n",
      "EPOCH 6 | mIOU 0.34318662529917116\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.822, lr=0.000469, step=2695\n",
      "47/47 01:24<00:00 \n",
      "EPOCH 7 | mIOU 0.34770855001480405\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.814, lr=0.00046, step=3080 \n",
      "47/47 01:09<00:00 \n",
      "EPOCH 8 | mIOU 0.35104913479845506\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.793, lr=0.00045, step=3465 \n",
      "47/47 01:08<00:00 \n",
      "EPOCH 9 | mIOU 0.3528597396701962\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.781, lr=0.000439, step=3850\n",
      "47/47 01:10<00:00 \n",
      "EPOCH 10 | mIOU 0.35980142272950416\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.796, lr=0.000427, step=4235\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 11 | mIOU 0.35474437891874844\n",
      "385/385 06:49<00:00 , loss=0.782, lr=0.000414, step=4620\n",
      "47/47 01:11<00:00 \n",
      "EPOCH 12 | mIOU 0.35506145277804685\n",
      "385/385 06:49<00:00 , loss=0.775, lr=0.0004, step=5005  \n",
      "47/47 01:11<00:00 \n",
      "EPOCH 13 | mIOU 0.3593548553809889\n",
      "385/385 06:49<00:00 , loss=0.763, lr=0.000385, step=5390\n",
      "47/47 01:14<00:00 \n",
      "EPOCH 14 | mIOU 0.3630236769513207\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.764, lr=0.00037, step=5775 \n",
      "47/47 01:13<00:00 \n",
      "EPOCH 15 | mIOU 0.36286539463847145\n",
      "385/385 06:50<00:00 , loss=0.768, lr=0.000354, step=6160\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 16 | mIOU 0.3660942499240282\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.726, lr=0.000337, step=6545\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 17 | mIOU 0.36408397569961837\n",
      "385/385 06:49<00:00 , loss=0.718, lr=0.00032, step=6930 \n",
      "47/47 01:07<00:00 \n",
      "EPOCH 18 | mIOU 0.36662321607504045\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.754, lr=0.000303, step=7315\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 19 | mIOU 0.36595012121773896\n",
      "385/385 06:49<00:00 , loss=0.737, lr=0.000286, step=7700\n",
      "47/47 01:08<00:00 \n",
      "EPOCH 20 | mIOU 0.36335583718107767\n",
      "385/385 06:50<00:00 , loss=0.755, lr=0.000268, step=8085\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 21 | mIOU 0.368604568444686\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.729, lr=0.00025, step=8470 \n",
      "47/47 01:09<00:00 \n",
      "EPOCH 22 | mIOU 0.3668559882793761\n",
      "385/385 06:49<00:00 , loss=0.729, lr=0.000232, step=8855\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 23 | mIOU 0.37034933050903235\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.698, lr=0.000214, step=9240\n",
      "47/47 01:07<00:00 \n",
      "EPOCH 24 | mIOU 0.37352641914086776\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.697, lr=0.000197, step=9625\n",
      "47/47 01:09<00:00 \n",
      "EPOCH 25 | mIOU 0.37189033869343996\n",
      "385/385 06:49<00:00 , loss=0.702, lr=0.00018, step=1e+4 \n",
      "47/47 01:08<00:00 \n",
      "EPOCH 26 | mIOU 0.3743718778123172\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.687, lr=0.000163, step=10395\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 27 | mIOU 0.3759890216023138\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.682, lr=0.000146, step=10780\n",
      "47/47 01:07<00:00 \n",
      "EPOCH 28 | mIOU 0.3756569559757716\n",
      "385/385 06:49<00:00 , loss=0.699, lr=0.00013, step=11165 \n",
      "47/47 01:07<00:00 \n",
      "EPOCH 29 | mIOU 0.37938318573970464\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.69, lr=0.000115, step=11550 \n",
      "47/47 01:07<00:00 \n",
      "EPOCH 30 | mIOU 0.37677363517996393\n",
      "385/385 06:49<00:00 , loss=0.665, lr=0.0001, step=11935  \n",
      "47/47 01:09<00:00 \n",
      "EPOCH 31 | mIOU 0.3788953353202252\n",
      "385/385 06:50<00:00 , loss=0.693, lr=8.63e-5, step=12320\n",
      "47/47 01:08<00:00 \n",
      "EPOCH 32 | mIOU 0.3777756170729621\n",
      "385/385 06:49<00:00 , loss=0.674, lr=7.32e-5, step=12705\n",
      "47/47 01:07<00:00 \n",
      "EPOCH 33 | mIOU 0.37775169463935354\n",
      "385/385 06:49<00:00 , loss=0.665, lr=6.11e-5, step=13090\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 34 | mIOU 0.3799219020762772\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.671, lr=4.99e-5, step=13475\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 35 | mIOU 0.37985422137606434\n",
      "385/385 06:49<00:00 , loss=0.684, lr=3.97e-5, step=13860\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 36 | mIOU 0.380761121006033\n",
      "SAVING BEST!\n",
      "385/385 06:48<00:00 , loss=0.659, lr=3.06e-5, step=14245\n",
      "47/47 01:06<00:00 \n",
      "EPOCH 37 | mIOU 0.3819841167959346\n",
      "SAVING BEST!\n",
      "385/385 06:49<00:00 , loss=0.67, lr=2.26e-5, step=14630 \n",
      "47/47 01:07<00:00 \n",
      "EPOCH 38 | mIOU 0.3814688387546169\n",
      "385/385 06:50<00:00 , loss=0.678, lr=1.58e-5, step=15015\n",
      "47/47 01:08<00:00 \n",
      "EPOCH 39 | mIOU 0.3804284667018447\n",
      "385/385 06:49<00:00 , loss=0.655, lr=1.01e-5, step=15400\n",
      "47/47 01:10<00:00 \n",
      "EPOCH 40 | mIOU 0.3809473821103326\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"convnext_large_exp_002_forward\"\n",
    "V = \"6\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760af7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3eda68c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 30293 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/EVA/EVA-02/seg/work_dirs/eva_02_large_exp_001/best_mIoU_iter_42735_mod.pth\n",
      "0/577 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "577/577 11:06<00:00 , loss=0.94, lr=9.99e-5, step=577 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 1 | mIOU 0.37183566282548675\n",
      "SAVING BEST!\n",
      "577/577 11:05<00:00 , loss=0.714, lr=9.95e-5, step=1154\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 2 | mIOU 0.37966429775665234\n",
      "SAVING BEST!\n",
      "577/577 11:03<00:00 , loss=0.68, lr=9.89e-5, step=1731 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 3 | mIOU 0.38229303898654876\n",
      "SAVING BEST!\n",
      "577/577 11:05<00:00 , loss=0.65, lr=9.8e-5, step=2308  \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 4 | mIOU 0.3732128773031071\n",
      "577/577 11:05<00:00 , loss=0.632, lr=9.68e-5, step=2885\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 5 | mIOU 0.38311551068446675\n",
      "SAVING BEST!\n",
      "577/577 11:03<00:00 , loss=0.627, lr=9.55e-5, step=3462\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 6 | mIOU 0.3862093982671588\n",
      "SAVING BEST!\n",
      "577/577 11:09<00:00 , loss=0.608, lr=9.39e-5, step=4039\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 7 | mIOU 0.3862251671391444\n",
      "SAVING BEST!\n",
      "577/577 11:03<00:00 , loss=0.608, lr=9.21e-5, step=4616\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 8 | mIOU 0.3822972952097645\n",
      "577/577 11:06<00:00 , loss=0.585, lr=9e-5, step=5193   \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 9 | mIOU 0.38538087188358905\n",
      "577/577 11:06<00:00 , loss=0.563, lr=8.78e-5, step=5770\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 10 | mIOU 0.3861627445574408\n",
      "577/577 11:07<00:00 , loss=0.586, lr=8.54e-5, step=6347\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 11 | mIOU 0.3907071024146933\n",
      "SAVING BEST!\n",
      "577/577 11:11<00:00 , loss=0.528, lr=8.27e-5, step=6924\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 12 | mIOU 0.3812915165004885\n",
      "577/577 11:05<00:00 , loss=0.576, lr=8e-5, step=7501   \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 13 | mIOU 0.3857210749593697\n",
      "92/577 01:47<09:20 , loss=0.506, lr=7.95e-5, step=7593"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"eva_02_large_exp_001\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb13cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f902347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 30575 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/EVA/EVA-02/seg/work_dirs/eva_02_large_exp_001/best_mIoU_iter_42735_mod.pth\n",
      "0/577 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "577/577 11:18<00:00 , loss=1.15, lr=3e-5, step=577\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 1 | mIOU 0.3433460833791337\n",
      "SAVING BEST!\n",
      "577/577 11:14<00:00 , loss=0.78, lr=2.98e-5, step=1154 \n",
      "47/47 00:52<00:00 \n",
      "EPOCH 2 | mIOU 0.37069679197854255\n",
      "SAVING BEST!\n",
      "577/577 11:15<00:00 , loss=0.712, lr=2.97e-5, step=1731\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 3 | mIOU 0.3810508181055007\n",
      "SAVING BEST!\n",
      "577/577 11:17<00:00 , loss=0.672, lr=2.94e-5, step=2308\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 4 | mIOU 0.3894530961690611\n",
      "SAVING BEST!\n",
      "577/577 11:23<00:00 , loss=0.645, lr=2.91e-5, step=2885\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 5 | mIOU 0.39014988746673956\n",
      "SAVING BEST!\n",
      "577/577 11:22<00:00 , loss=0.641, lr=2.86e-5, step=3462\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 6 | mIOU 0.3951589875805307\n",
      "SAVING BEST!\n",
      "577/577 11:21<00:00 , loss=0.62, lr=2.82e-5, step=4039 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 7 | mIOU 0.3915535430147727\n",
      "577/577 11:19<00:00 , loss=0.616, lr=2.76e-5, step=4616\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 8 | mIOU 0.3972900136128314\n",
      "SAVING BEST!\n",
      "577/577 11:19<00:00 , loss=0.596, lr=2.7e-5, step=5193 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 9 | mIOU 0.3945869693833989\n",
      "577/577 11:16<00:00 , loss=0.579, lr=2.63e-5, step=5770\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 10 | mIOU 0.3957519483254101\n",
      "577/577 11:16<00:00 , loss=0.604, lr=2.56e-5, step=6347\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 11 | mIOU 0.39472236911492986\n",
      "577/577 11:18<00:00 , loss=0.546, lr=2.48e-5, step=6924\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 12 | mIOU 0.39399626089373013\n",
      "577/577 11:18<00:00 , loss=0.6, lr=2.4e-5, step=7501   \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 13 | mIOU 0.3949476128131135\n",
      "577/577 11:17<00:00 , loss=0.558, lr=2.31e-5, step=8078\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 14 | mIOU 0.3967216178421676\n",
      "577/577 11:18<00:00 , loss=0.574, lr=2.22e-5, step=8655\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 15 | mIOU 0.39486310699259647\n",
      "577/577 11:20<00:00 , loss=0.54, lr=2.12e-5, step=9232 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 16 | mIOU 0.3972236776321893\n",
      "577/577 11:19<00:00 , loss=0.549, lr=2.02e-5, step=9809\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 17 | mIOU 0.3948720436724303\n",
      "577/577 11:19<00:00 , loss=0.532, lr=1.92e-5, step=10386\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 18 | mIOU 0.3979879889966009\n",
      "SAVING BEST!\n",
      "577/577 11:20<00:00 , loss=0.519, lr=1.82e-5, step=10963\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 19 | mIOU 0.3994326887362608\n",
      "SAVING BEST!\n",
      "577/577 11:16<00:00 , loss=0.529, lr=1.71e-5, step=11540\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 20 | mIOU 0.3979340161617052\n",
      "577/577 11:18<00:00 , loss=0.517, lr=1.61e-5, step=12117\n",
      "47/47 00:57<00:00 \n",
      "EPOCH 21 | mIOU 0.40090543141563534\n",
      "SAVING BEST!\n",
      "577/577 11:23<00:00 , loss=0.506, lr=1.5e-5, step=12694 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 22 | mIOU 0.40171641391650637\n",
      "SAVING BEST!\n",
      "577/577 11:25<00:00 , loss=0.506, lr=1.39e-5, step=13271\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 23 | mIOU 0.40043553589253456\n",
      "577/577 11:19<00:00 , loss=0.498, lr=1.29e-5, step=13848\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 24 | mIOU 0.40072963835501024\n",
      "577/577 11:19<00:00 , loss=0.496, lr=1.18e-5, step=14425\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 25 | mIOU 0.39941954342189345\n",
      "577/577 11:17<00:00 , loss=0.506, lr=1.08e-5, step=15002\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 26 | mIOU 0.39943863700940285\n",
      "577/577 11:20<00:00 , loss=0.479, lr=9.76e-6, step=15579\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 27 | mIOU 0.39955357210934545\n",
      "577/577 11:19<00:00 , loss=0.483, lr=8.77e-6, step=16156\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 28 | mIOU 0.4007680577308504\n",
      "577/577 11:20<00:00 , loss=0.5, lr=7.81e-6, step=16733  \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 29 | mIOU 0.4017971924400271\n",
      "SAVING BEST!\n",
      "577/577 11:24<00:00 , loss=0.476, lr=6.89e-6, step=17310\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 30 | mIOU 0.4042650934146854\n",
      "SAVING BEST!\n",
      "577/577 11:17<00:00 , loss=0.481, lr=6.01e-6, step=17887\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 31 | mIOU 0.4024872225577021\n",
      "577/577 11:17<00:00 , loss=0.474, lr=5.18e-6, step=18464\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 32 | mIOU 0.4042257899036505\n",
      "577/577 11:17<00:00 , loss=0.494, lr=4.39e-6, step=19041\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 33 | mIOU 0.40377392569478754\n",
      "577/577 11:17<00:00 , loss=0.467, lr=3.66e-6, step=19618\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 34 | mIOU 0.4013236092353763\n",
      "577/577 11:17<00:00 , loss=0.468, lr=2.99e-6, step=20195\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 35 | mIOU 0.40309469966118294\n",
      "577/577 11:19<00:00 , loss=0.467, lr=2.38e-6, step=20772\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 36 | mIOU 0.4028504049257296\n",
      "577/577 11:22<00:00 , loss=0.484, lr=1.83e-6, step=21349\n",
      "47/47 00:55<00:00 \n",
      "EPOCH 37 | mIOU 0.40285947829442437\n",
      "577/577 11:19<00:00 , loss=0.467, lr=1.36e-6, step=21926\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 38 | mIOU 0.4029592635029\n",
      "577/577 11:19<00:00 , loss=0.454, lr=9.46e-7, step=22503\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 39 | mIOU 0.40287321525755465\n",
      "577/577 11:19<00:00 , loss=0.449, lr=6.08e-7, step=23080\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 40 | mIOU 0.40232305763614457\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"eva_02_large_exp_001\"\n",
    "V = \"2\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbe9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdf8c5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 30971 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/EVA/EVA-02/seg/work_dirs/eva_02_large_exp_001/best_mIoU_iter_42735_mod.pth\n",
      "0/577 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "577/577 11:08<00:00 , loss=1.41, lr=9.99e-6, step=577\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 1 | mIOU 0.31411448189206576\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.918, lr=9.95e-6, step=1154\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 2 | mIOU 0.340688991497974\n",
      "SAVING BEST!\n",
      "577/577 11:05<00:00 , loss=0.807, lr=9.89e-6, step=1731\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 3 | mIOU 0.3597774827765631\n",
      "SAVING BEST!\n",
      "577/577 11:04<00:00 , loss=0.742, lr=9.8e-6, step=2308 \n",
      "47/47 00:52<00:00 \n",
      "EPOCH 4 | mIOU 0.3719928439698481\n",
      "SAVING BEST!\n",
      "577/577 11:04<00:00 , loss=0.705, lr=9.68e-6, step=2885\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 5 | mIOU 0.37668361380163246\n",
      "SAVING BEST!\n",
      "577/577 11:05<00:00 , loss=0.693, lr=9.55e-6, step=3462\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 6 | mIOU 0.3834604341000963\n",
      "SAVING BEST!\n",
      "577/577 11:04<00:00 , loss=0.671, lr=9.39e-6, step=4039\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 7 | mIOU 0.38552042930711417\n",
      "SAVING BEST!\n",
      "577/577 11:04<00:00 , loss=0.667, lr=9.21e-6, step=4616\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 8 | mIOU 0.38988293493687365\n",
      "SAVING BEST!\n",
      "577/577 11:04<00:00 , loss=0.648, lr=9e-6, step=5193   \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 9 | mIOU 0.3925667107913514\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.63, lr=8.78e-6, step=5770 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 10 | mIOU 0.3952749465308142\n",
      "SAVING BEST!\n",
      "577/577 11:05<00:00 , loss=0.658, lr=8.54e-6, step=6347\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 11 | mIOU 0.39320195503655425\n",
      "577/577 11:06<00:00 , loss=0.598, lr=8.27e-6, step=6924\n",
      "47/47 00:52<00:00 \n",
      "EPOCH 12 | mIOU 0.395517565071679\n",
      "SAVING BEST!\n",
      "577/577 11:04<00:00 , loss=0.653, lr=8e-6, step=7501   \n",
      "47/47 00:52<00:00 \n",
      "EPOCH 13 | mIOU 0.39585813019982763\n",
      "SAVING BEST!\n",
      "577/577 11:03<00:00 , loss=0.613, lr=7.7e-6, step=8078 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 14 | mIOU 0.39726020818062774\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.63, lr=7.4e-6, step=8655  \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 15 | mIOU 0.3970654652992885\n",
      "577/577 11:06<00:00 , loss=0.599, lr=7.08e-6, step=9232\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 16 | mIOU 0.3983266498286132\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.611, lr=6.75e-6, step=9809\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 17 | mIOU 0.3977029598659432\n",
      "577/577 11:05<00:00 , loss=0.593, lr=6.41e-6, step=10386\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 18 | mIOU 0.3963727479071226\n",
      "577/577 11:05<00:00 , loss=0.581, lr=6.06e-6, step=10963\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 19 | mIOU 0.397440288310846\n",
      "577/577 11:05<00:00 , loss=0.591, lr=5.71e-6, step=11540\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 20 | mIOU 0.397718033533085\n",
      "577/577 11:06<00:00 , loss=0.583, lr=5.36e-6, step=12117\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 21 | mIOU 0.39743871798562097\n",
      "577/577 11:05<00:00 , loss=0.575, lr=5e-6, step=12694   \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 22 | mIOU 0.3995407343863387\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.579, lr=4.64e-6, step=13271\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 23 | mIOU 0.39995454744390097\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.57, lr=4.29e-6, step=13848 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 24 | mIOU 0.40004083709603583\n",
      "SAVING BEST!\n",
      "577/577 11:08<00:00 , loss=0.565, lr=3.94e-6, step=14425\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 25 | mIOU 0.39901515487701433\n",
      "577/577 11:09<00:00 , loss=0.581, lr=3.59e-6, step=15002\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 26 | mIOU 0.39946023490989624\n",
      "577/577 11:08<00:00 , loss=0.555, lr=3.25e-6, step=15579\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 27 | mIOU 0.3997806420934228\n",
      "577/577 11:08<00:00 , loss=0.561, lr=2.92e-6, step=16156\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 28 | mIOU 0.3995280932470288\n",
      "577/577 11:09<00:00 , loss=0.575, lr=2.6e-6, step=16733 \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 29 | mIOU 0.3997800878534867\n",
      "577/577 11:08<00:00 , loss=0.553, lr=2.3e-6, step=17310 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 30 | mIOU 0.4001854868217378\n",
      "SAVING BEST!\n",
      "577/577 11:09<00:00 , loss=0.561, lr=2e-6, step=17887   \n",
      "47/47 00:53<00:00 \n",
      "EPOCH 31 | mIOU 0.4006912111447319\n",
      "SAVING BEST!\n",
      "577/577 11:09<00:00 , loss=0.553, lr=1.73e-6, step=18464\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 32 | mIOU 0.4009726367104608\n",
      "SAVING BEST!\n",
      "577/577 11:07<00:00 , loss=0.576, lr=1.46e-6, step=19041\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 33 | mIOU 0.4012968736568848\n",
      "SAVING BEST!\n",
      "577/577 11:06<00:00 , loss=0.546, lr=1.22e-6, step=19618\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 34 | mIOU 0.40094723788597353\n",
      "577/577 11:08<00:00 , loss=0.549, lr=9.97e-7, step=20195\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 35 | mIOU 0.40085398136651595\n",
      "577/577 11:07<00:00 , loss=0.552, lr=7.94e-7, step=20772\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 36 | mIOU 0.40043264326600286\n",
      "577/577 11:09<00:00 , loss=0.569, lr=6.12e-7, step=21349\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 37 | mIOU 0.40055880723199105\n",
      "577/577 11:07<00:00 , loss=0.553, lr=4.52e-7, step=21926\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 38 | mIOU 0.4008893734366596\n",
      "577/577 11:09<00:00 , loss=0.536, lr=3.15e-7, step=22503\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 39 | mIOU 0.4006987341584726\n",
      "577/577 11:07<00:00 , loss=0.531, lr=2.03e-7, step=23080\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 40 | mIOU 0.4003214113973746\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"eva_02_large_exp_001\"\n",
    "V = \"3\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972c185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58817ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 31075 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/EVA/EVA-02/seg/work_dirs/eva_02_large_exp_001/best_mIoU_iter_42735_mod.pth\n",
      "0/577 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "577/577 11:14<00:00 , loss=1.15, lr=2.98e-5, step=577\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 1 | mIOU 0.3435945275969947\n",
      "SAVING BEST!\n",
      "577/577 11:16<00:00 , loss=0.78, lr=2.94e-5, step=1154 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 2 | mIOU 0.3705703371679875\n",
      "SAVING BEST!\n",
      "577/577 11:16<00:00 , loss=0.712, lr=2.86e-5, step=1731\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 3 | mIOU 0.3808283399765951\n",
      "SAVING BEST!\n",
      "577/577 11:15<00:00 , loss=0.672, lr=2.76e-5, step=2308\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 4 | mIOU 0.38861752330729477\n",
      "SAVING BEST!\n",
      "577/577 11:13<00:00 , loss=0.645, lr=2.63e-5, step=2885\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 5 | mIOU 0.39017804481618656\n",
      "SAVING BEST!\n",
      "577/577 11:13<00:00 , loss=0.64, lr=2.48e-5, step=3462 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 6 | mIOU 0.39497943584159\n",
      "SAVING BEST!\n",
      "577/577 11:17<00:00 , loss=0.62, lr=2.31e-5, step=4039 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 7 | mIOU 0.39321406298395367\n",
      "577/577 11:18<00:00 , loss=0.615, lr=2.12e-5, step=4616\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 8 | mIOU 0.39658029044892584\n",
      "SAVING BEST!\n",
      "577/577 11:13<00:00 , loss=0.595, lr=1.92e-5, step=5193\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 9 | mIOU 0.3956494089964285\n",
      "577/577 11:16<00:00 , loss=0.579, lr=1.71e-5, step=5770\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 10 | mIOU 0.3975060747454412\n",
      "SAVING BEST!\n",
      "577/577 11:16<00:00 , loss=0.605, lr=1.5e-5, step=6347 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 11 | mIOU 0.3952890284995503\n",
      "577/577 11:17<00:00 , loss=0.548, lr=1.29e-5, step=6924\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 12 | mIOU 0.3965251803190071\n",
      "577/577 11:18<00:00 , loss=0.6, lr=1.08e-5, step=7501  \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 13 | mIOU 0.3968112604126928\n",
      "577/577 11:20<00:00 , loss=0.56, lr=8.77e-6, step=8078 \n",
      "47/47 00:54<00:00 \n",
      "EPOCH 14 | mIOU 0.39869885256896315\n",
      "SAVING BEST!\n",
      "577/577 11:18<00:00 , loss=0.577, lr=6.89e-6, step=8655\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 15 | mIOU 0.3973542816100455\n",
      "577/577 11:17<00:00 , loss=0.545, lr=5.18e-6, step=9232\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 16 | mIOU 0.3994841208468448\n",
      "SAVING BEST!\n",
      "577/577 11:15<00:00 , loss=0.559, lr=3.66e-6, step=9809\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 17 | mIOU 0.39978288936245954\n",
      "SAVING BEST!\n",
      "577/577 11:15<00:00 , loss=0.545, lr=2.38e-6, step=10386\n",
      "47/47 00:54<00:00 \n",
      "EPOCH 18 | mIOU 0.39918288806193136\n",
      "577/577 11:17<00:00 , loss=0.534, lr=1.36e-6, step=10963\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 19 | mIOU 0.3997047452920225\n",
      "577/577 11:15<00:00 , loss=0.546, lr=6.08e-7, step=11540\n",
      "47/47 00:53<00:00 \n",
      "EPOCH 20 | mIOU 0.3998855265240161\n",
      "SAVING BEST!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"eva_02_large_exp_001\"\n",
    "V = \"4\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6818b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2d2365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 31824 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/EVA/EVA-02/seg/work_dirs/eva_02_large_exp_001/best_mIoU_iter_42735_mod.pth\n",
      "0/651 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "651/651 12:47<00:00 , loss=0.547, lr=9.89e-6, step=651\n",
      "5/5 00:08<00:00 \n",
      "EPOCH 1 | mIOU 0.3772520672407393\n",
      "SAVING BEST!\n",
      "651/651 12:47<00:00 , loss=0.547, lr=9.57e-6, step=1302\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 2 | mIOU 0.37768576727738895\n",
      "SAVING BEST!\n",
      "651/651 12:47<00:00 , loss=0.548, lr=9.05e-6, step=1953\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 3 | mIOU 0.3813432791834011\n",
      "SAVING BEST!\n",
      "651/651 12:45<00:00 , loss=0.558, lr=8.35e-6, step=2604\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 4 | mIOU 0.3777474660744989\n",
      "651/651 12:45<00:00 , loss=0.548, lr=7.5e-6, step=3255 \n",
      "5/5 00:07<00:00 \n",
      "EPOCH 5 | mIOU 0.374952283729554\n",
      "651/651 12:45<00:00 , loss=0.555, lr=6.55e-6, step=3906\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 6 | mIOU 0.38110478414026533\n",
      "651/651 12:46<00:00 , loss=0.561, lr=5.52e-6, step=4557\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 7 | mIOU 0.37842248590957184\n",
      "651/651 12:43<00:00 , loss=0.546, lr=4.48e-6, step=5208\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 8 | mIOU 0.38256658780829683\n",
      "SAVING BEST!\n",
      "651/651 12:42<00:00 , loss=0.546, lr=3.45e-6, step=5859\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 9 | mIOU 0.3825531784861692\n",
      "651/651 12:43<00:00 , loss=0.552, lr=2.5e-6, step=6510 \n",
      "5/5 00:07<00:00 \n",
      "EPOCH 10 | mIOU 0.38201727690665344\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"eva_02_large_exp_001\"\n",
    "V = \"5\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03909af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6bd1592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 31824 bytes to train1.py\n",
      "/home/harshit/anaconda3/envs/dacl10k_mmseg_t1/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "load checkpoint from local path: /mnt/md0/dacl10k/AAA_MMSEG/EVA/EVA-02/seg/work_dirs/eva_02_large_exp_001/best_mIoU_iter_42735_mod.pth\n",
      "0/651 00:00<? [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "651/651 12:45<00:00 , loss=0.494, lr=9.97e-6, step=651\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 1 | mIOU 0.37169465513311717\n",
      "SAVING BEST!\n",
      "651/651 12:41<00:00 , loss=0.492, lr=9.89e-6, step=1302\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 2 | mIOU 0.37081895140980026\n",
      "651/651 12:43<00:00 , loss=0.494, lr=9.76e-6, step=1953\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 3 | mIOU 0.37491258463430166\n",
      "SAVING BEST!\n",
      "651/651 12:42<00:00 , loss=0.504, lr=9.57e-6, step=2604\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 4 | mIOU 0.372072054779059\n",
      "651/651 12:43<00:00 , loss=0.497, lr=9.33e-6, step=3255\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 5 | mIOU 0.3720218958725271\n",
      "651/651 12:43<00:00 , loss=0.503, lr=9.05e-6, step=3906\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 6 | mIOU 0.37332408484071494\n",
      "651/651 12:42<00:00 , loss=0.511, lr=8.72e-6, step=4557\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 7 | mIOU 0.37118676129458966\n",
      "651/651 12:42<00:00 , loss=0.497, lr=8.35e-6, step=5208\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 8 | mIOU 0.37334301666797776\n",
      "651/651 12:44<00:00 , loss=0.495, lr=7.94e-6, step=5859\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 9 | mIOU 0.37330728084045023\n",
      "651/651 12:43<00:00 , loss=0.502, lr=7.5e-6, step=6510 \n",
      "5/5 00:07<00:00 \n",
      "EPOCH 10 | mIOU 0.3738122101382989\n",
      "651/651 12:42<00:00 , loss=0.481, lr=7.03e-6, step=7161\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 11 | mIOU 0.3733895717727903\n",
      "651/651 12:42<00:00 , loss=0.489, lr=6.55e-6, step=7812\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 12 | mIOU 0.3687601835166692\n",
      "651/651 12:42<00:00 , loss=0.486, lr=6.04e-6, step=8463\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 13 | mIOU 0.36621785086432573\n",
      "651/651 12:43<00:00 , loss=0.472, lr=5.52e-6, step=9114\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 14 | mIOU 0.3707725097670367\n",
      "651/651 12:43<00:00 , loss=0.486, lr=5e-6, step=9765   \n",
      "5/5 00:07<00:00 \n",
      "EPOCH 15 | mIOU 0.370144669902756\n",
      "651/651 12:45<00:00 , loss=0.488, lr=4.48e-6, step=10416\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 16 | mIOU 0.36687010067613113\n",
      "651/651 12:43<00:00 , loss=0.463, lr=3.96e-6, step=11067\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 17 | mIOU 0.3695765137329305\n",
      "651/651 12:43<00:00 , loss=0.468, lr=3.45e-6, step=11718\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 18 | mIOU 0.3691588162395515\n",
      "651/651 12:44<00:00 , loss=0.466, lr=2.97e-6, step=12369\n",
      "5/5 00:07<00:00 \n",
      "EPOCH 19 | mIOU 0.37147901237231534\n",
      "651/651 12:45<00:00 , loss=0.468, lr=2.5e-6, step=13020 \n",
      "5/5 00:07<00:00 \n",
      "EPOCH 20 | mIOU 0.3705349251529888\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DIR = \"/mnt/md0/dacl10k/AAA_MMSEG/TRY1_SEG/\"\n",
    "NAME = \"eva_02_large_exp_001\"\n",
    "V = \"2_full\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !source /home/harshit/anaconda3/bin/activate dacl10k_mmseg_t1 && CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671305d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3274e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200a486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8eb232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e75d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.37472705976106224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d053834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61026713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I LEFT COARSEDROPOUT IN V5 BYMISTAKE\n",
    "# V6 IS V5 WITHOUT THE COARSEDROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5b54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f024b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368158c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacl10k_mmseg_t1",
   "language": "python",
   "name": "dacl10k_mmseg_t1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
